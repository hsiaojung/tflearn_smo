{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from dataset_utils import _dataset_exists, _get_filenames_and_classes, write_label_file, _convert_dataset\n",
    "\n",
    "## python create_tfrecord.py  --tfrecord_filename=200label --dataset_dir=/dataset-imagenet/\n",
    "                           ## --num_shards=21 --validation_size=0.2 --random_seed=1\n",
    "'''\n",
    "#====================================================DEFINE YOUR ARGUMENTS=======================================================================\n",
    "flags = tf.app.flags\n",
    "#State your dataset directory\n",
    "flags.DEFINE_string('dataset_dir', None, 'String: Your dataset directory')\n",
    "# The number of images in the validation set. You would have to know the total number of examples in advance. This is essentially your evaluation dataset.\n",
    "# The number of shards to split the dataset into\n",
    "flags.DEFINE_integer('num_shards', 2, 'Int: Number of shards to split the TFRecord files')\n",
    "\n",
    "# Seed for repeatability.\n",
    "flags.DEFINE_integer('random_seed', 0, 'Int: Random seed to use for repeatability.')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('tfrecord_filename', None, 'String: The output filename to name your TFRecord file')\n",
    "FLAGS = flags.FLAGS\n",
    "'''\n",
    "\n",
    "tfrecord_filename='smotfrecord'\n",
    "dataset_dir='/home/aewin/work/anaconda3/code/V001/smodata/'\n",
    "#datasets_dir = '/home/aewin/work/anaconda3/code/V001/smorking'\n",
    "num_shards=2\n",
    "validation_size=0.2 \n",
    "random_seed=1\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    #==============================================================CHECKS==========================================================================\n",
    "    #Check if there is a tfrecord_filename entered\n",
    "    if not FLAGS.tfrecord_filename:\n",
    "        raise ValueError('tfrecord_filename is empty. Please state a tfrecord_filename argument.')\n",
    "\n",
    "    #Check if there is a dataset directory entered\n",
    "    if not FLAGS.dataset_dir:\n",
    "        raise ValueError('dataset_dir is empty. Please state a dataset_dir argument.')\n",
    "\n",
    "    #If the TFRecord files already exist in the directory, then exit without creating the files again\n",
    "    if _dataset_exists(dataset_dir = FLAGS.dataset_dir, _NUM_SHARDS = FLAGS.num_shards, output_filename = FLAGS.tfrecord_filename):\n",
    "        print 'Dataset files already exist. Exiting without re-creating them.'\n",
    "        return None\n",
    "    #==============================================================END OF CHECKS===================================================================\n",
    "    '''\n",
    "    #Get a list of photo_filenames like ['123.jpg', '456.jpg'...] and a list of sorted class names from parsing the subdirectories.\n",
    "    photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)\n",
    "    print(class_names,len(class_names))\n",
    "    print(type(class_names))\n",
    "    print ('\\n Checking...0')\n",
    "    #Refer each of the class name to a specific integer number for predictions later\n",
    "    class_id = [int(i) for i in range(len(class_names))]\n",
    "    print(class_id,len(class_id))\n",
    "    print(type(class_id))\n",
    "    print(class_id)\n",
    "    \n",
    "    class_names_to_ids = dict(zip(class_names, class_id))\n",
    "    #class_names_to_ids = dict(zip(class_names, range(len(class_names))))\n",
    "    print(class_names_to_ids)\n",
    "    #Find the number of validation examples we need\n",
    "    exit(1)\n",
    "    num_validation = int(validation_size * len(photo_filenames))\n",
    "    print ('\\n Checking')\n",
    "    print (class_names_to_ids)\n",
    "    print (len(class_names_to_ids))\n",
    "    \n",
    "    \n",
    "    sys.stdout.write('\\n num_validation: %d, num class number %d \\n' % ( num_validation, len(class_names) ))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Divide the training datasets into train and test:\n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(photo_filenames)\n",
    "    training_filenames = photo_filenames[num_validation:]\n",
    "    validation_filenames = photo_filenames[:num_validation]\n",
    "    print ('----------------')\n",
    "    print (training_filenames)\n",
    "    print (tfrecord_filename)\n",
    "    print (num_shards)\n",
    "    print ('----------------')\n",
    "    # First, convert the training and validation sets.\n",
    "    _convert_dataset('train', training_filenames, class_names_to_ids,\n",
    "                     dataset_dir = dataset_dir, tfrecord_filename = tfrecord_filename, _NUM_SHARDS = num_shards)\n",
    "    \n",
    "    \n",
    "    _convert_dataset('validation', validation_filenames, class_names_to_ids,\n",
    "                     dataset_dir = dataset_dir, tfrecord_filename = tfrecord_filename, _NUM_SHARDS = num_shards)\n",
    "\n",
    "    # Finally, write the labels file:\n",
    "    labels_to_class_names = dict(zip(class_id, class_names))\n",
    "    #labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n",
    "    write_label_file(labels_to_class_names, dataset_dir)\n",
    "    \n",
    "    print ('\\nFinished converting the %s dataset!' % (tfrecord_filename))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib import slim\n",
    "import os\n",
    "import time\n",
    "file_pattern = 'smotfrecord_train_%s.tfrecord'\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=file_pattern):\n",
    "    '''\n",
    "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
    "    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "    Your file_pattern is very important in locating the files later. \n",
    "    INPUTS:\n",
    "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "    '''\n",
    "\n",
    "    #First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation']:\n",
    "        raise ValueError('The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name))\n",
    "\n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = file_pattern\n",
    "\n",
    "    #Count the total number of examples in all of these shard\n",
    "    num_samples = 0\n",
    "    '''\n",
    "    file_pattern_for_counting = '200label_' + split_name\n",
    "    tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\n",
    "    print(file_pattern_for_counting)\n",
    "    '''\n",
    "    tfrecords_to_count = [os.path.join(dataset_dir,file) for file in os.listdir(dataset_dir)]\n",
    "         \n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "            num_samples += 1\n",
    "\n",
    "    \n",
    "    print(tfrecords_to_count)\n",
    "    \n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "      'image/class/label': tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "    'image': slim.tfexample_decoder.Image(),\n",
    "    'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "    #Create the labels_to_name file\n",
    "    labels_to_name_dict = labels_to_name\n",
    "    \n",
    "    print(tfrecords_to_count)\n",
    "    \n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = tfrecords_to_count,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        num_readers = 4,\n",
    "        num_samples = num_samples,\n",
    "        num_classes = num_classes,\n",
    "        labels_to_name = labels_to_name_dict,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "    \n",
    "    \n",
    "    print(dataset.data_sources)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "get_split('train', datasets_data_dir, file_pattern=file_pattern)\n",
    "#file_pattern = 'smotfrecord_validation_%s.tfrecord'\n",
    "#get_split('validation', datasets_data_dir, file_pattern=file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import flowers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "with tf.Graph().as_default(): \n",
    "    dataset = get_split('train', dataset_dir)\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32, common_queue_min=1)\n",
    "    image, label = data_provider.get(['image', 'label'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
