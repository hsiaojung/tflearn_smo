{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this setcion code is for reading picture both smo and nosmo ,and convert them to tfRecode format! ###!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://kwotsin.github.io/tech/2017/02/11/transfer-learning.html\n",
    "\n",
    "#================ DATASET INFORMATION ======================\n",
    "#State dataset directory where the tfrecord files are located\n",
    "dataset_dir = '.'\n",
    "\n",
    "#State where your log file is at. If it doesn't exist, create it.\n",
    "log_dir = './out001/'\n",
    "\n",
    "#State where your checkpoint file is\n",
    "checkpoint_file = './inception_resnet_v2_2016_08_30.ckpt'\n",
    "\n",
    "#State the image size you're resizing your images to. We will use the default inception size of 299.\n",
    "image_size = 299\n",
    "\n",
    "#State the number of classes to predict:\n",
    "num_classes = 2\n",
    "\n",
    "#State the labels file and read it\n",
    "\n",
    "labels_file ='./labels.txt'\n",
    "labels = open(labels_file, 'r')\n",
    "\n",
    "#Create a dictionary to refer each label to their string name\n",
    "labels_to_name = {}\n",
    "\n",
    "# a dictionary of datasets_data_dir name\n",
    "#datasets_data_dir = '/home/aewin/work/anaconda3/code/models/research/slim/smorking'\n",
    "datasets_data_dir = '/home/aewin/work/anaconda3/code/V001/smodata/t'\n",
    "\n",
    "#================= TRAINING INFORMATION ==================\n",
    "#State the number of epochs to train\n",
    "num_epochs = 15 #3 mins every time\n",
    "\n",
    "#State your batch size\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "#Create a dictionary to refer each label to their string name\n",
    "labels_to_name = {}\n",
    "for line in labels:\n",
    "    label, string_name = line.split(':')\n",
    "    string_name = string_name[:-1] #Remove newline\n",
    "    labels_to_name[int(label)] = string_name\n",
    "\n",
    "\n",
    "\n",
    "#Learning rate information and configuration (Up to you to experiment)\n",
    "initial_learning_rate = 0.0002\n",
    "learning_rate_decay_factor = 0.7\n",
    "num_epochs_before_decay = 2\n",
    "file_pattern = 'smotfrecord_train_%s.tfrecord'\n",
    "\n",
    "\n",
    "items_to_descriptions = {\n",
    "    'image': 'A 3-channel RGB coloured flower image that is either tulips, sunflowers, roses, dandelion, or daisy.',\n",
    "    'label': 'A label that is as such -- 0:nosmo, 1:smo'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib import slim\n",
    "import os\n",
    "import time\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=file_pattern):\n",
    "    '''\n",
    "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
    "    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "    Your file_pattern is very important in locating the files later. \n",
    "    INPUTS:\n",
    "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "    '''\n",
    "\n",
    "    #First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation']:\n",
    "        raise ValueError('The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name))\n",
    "\n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = file_pattern\n",
    "\n",
    "    #Count the total number of examples in all of these shard\n",
    "    num_samples = 0\n",
    "    '''\n",
    "    file_pattern_for_counting = '200label_' + split_name\n",
    "    tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\n",
    "    print(file_pattern_for_counting)\n",
    "    '''\n",
    "    tfrecords_to_count = [os.path.join(dataset_dir,file) for file in os.listdir(dataset_dir)]\n",
    "         \n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "            num_samples += 1\n",
    "\n",
    "    \n",
    "    print(tfrecords_to_count)\n",
    "    \n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "      'image/class/label': tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "    'image': slim.tfexample_decoder.Image(),\n",
    "    'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "    #Create the labels_to_name file\n",
    "    labels_to_name_dict = labels_to_name\n",
    "    \n",
    "    print(tfrecords_to_count)\n",
    "\n",
    "    \n",
    "    \n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = tfrecords_to_count,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        num_readers = 4,\n",
    "        num_samples = num_samples,\n",
    "        num_classes = num_classes,\n",
    "        labels_to_name = labels_to_name_dict,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(dataset.data_sources)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "#get_split('train', datasets_data_dir, file_pattern=file_pattern)\n",
    "#file_pattern = 'smotfrecord_validation_%s.tfrecord'\n",
    "#get_split('validation', datasets_data_dir, file_pattern=file_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from datasets import flowers\n",
    "import setdate #from fllower.py\n",
    "import inception_preprocessing\n",
    "\n",
    "\n",
    "def load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=True):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "\n",
    "    '''\n",
    "    #First create the data_provider object\n",
    "    '''   \n",
    "    参数说明如下：\n",
    "    dataset：Dataset 类实例\n",
    "    num_readers：并行阅读器数量\n",
    "    reader_kwargs=None：阅读器关键配置字典\n",
    "    shuffle：是否打乱\n",
    "    num_epochs：每个数据源被读取的次数，如果设为None数据将会被无限循环的读取\n",
    "    common_queue_capacity：读取数据队t列的容量，默认为256\n",
    "    common_queue_min：读取数据队列的最小容量\n",
    "    record_key：（不是很理解）\n",
    "    seed=None：打乱是的种子\n",
    "    scope=None：范围\n",
    "    ''' \n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        common_queue_capacity = 24 + 3 * batch_size,\n",
    "        common_queue_min = 24)\n",
    "\n",
    "    #Obtain the raw image using the get method\n",
    "    raw_image, label = data_provider.get(['image', 'label'])\n",
    "\n",
    "    '''\n",
    "    with tf.Session() as sess:    \n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in range(7):\n",
    "                np_image, np_label = sess.run([raw_image, label])\n",
    "                print(np_label)\n",
    "                height, width, _ = np_image.shape\n",
    "                #class_name = name = dataset.labels_to_names[np_label]\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(np_image)\n",
    "                #plt.title('%s, %d x %d' % (name, height, width))\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "    '''\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\n",
    "    \n",
    "    #As for the raw images, we just do a simple reshape to batch it up\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    images, raw_images, labels = tf.train.batch(\n",
    "        [image, raw_image, label],\n",
    "        batch_size = batch_size,\n",
    "        num_threads = 4,\n",
    "        capacity = 4 * batch_size,\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return images, raw_images, labels\n",
    "\n",
    "load_batch(get_split('train', datasets_data_dir, file_pattern=file_pattern)\n",
    ", batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print (\"Current date and time : \")\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from datasets import flowers\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib import slim\n",
    "import os\n",
    "from inception_resnet_v2v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "import time\n",
    "#https://blog.gtwang.org/programming/tensorflow-read-write-tfrecords-data-format-tutorial/\n",
    "#实例化了一个类，一个用于 tensorflow 计算和表示用的数据流图\n",
    "'''  WHY using  tf.Graph().as_default():!!!!!!!!!!!!!!!\n",
    "   Since a default graph is always registered, every op and variable is placed into the default graph. \n",
    "   The statement of  tf.Graph().as_default(): , however,creates a new graph and places everything (declared inside its scope) into this graph.\n",
    "   If the graph is the only graph, it's useless. \n",
    "   But it's a good practice because if you start to work with many graphs it's easier to understand where ops and \n",
    "   vars are placed. \n",
    "   Since this statement costs you nothing, it's better to write it anyway. \n",
    "   Just to be sure that if you refactor the code in the future, \n",
    "   the operations defined belong to the graph you choose initially\n",
    "'''\n",
    "def run():\n",
    "    with tf.Graph().as_default(): \n",
    "  \n",
    "        \n",
    "        #指定獲取有“train”的資料\n",
    "        #dataset = get_split2('train',  './tf_train/','smorking_train_*.tfrecords')\n",
    "        #dataset = get_split('train',  './tf_train/','smorking_train_*.tfrecords')\n",
    "        \n",
    "        dataset = get_split('train', datasets_data_dir, file_pattern=file_pattern)\n",
    "        image, _, labels = load_batch(dataset, batch_size=batch_size)\n",
    "\n",
    " \n",
    "        #Know the number steps to take before decaying the learning rate and batches per epoch\n",
    "        num_batches_per_epoch = int(dataset.num_samples / batch_size)\n",
    "        num_steps_per_epoch = num_batches_per_epoch #Because one step is one batch processed\n",
    "        decay_steps = int(num_epochs_before_decay * num_steps_per_epoch)\n",
    "\n",
    "        #Create the model inference\n",
    "        with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "\n",
    "            logits, end_points = inception_resnet_v2(image, num_classes = dataset.num_classes, is_training = True)\n",
    "\n",
    "        #Define the scopes that you want to exclude for restoration\n",
    "        exclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']\n",
    "        variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n",
    "\n",
    "        #Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)\n",
    "        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "\n",
    "        #Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n",
    "        total_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well\n",
    "\n",
    "        #Create the global step for monitoring the learning_rate and training.\n",
    "        global_step = get_or_create_global_step()\n",
    "        #Define your exponentially decaying learning rate\n",
    "        lr = tf.train.exponential_decay(\n",
    "            learning_rate = initial_learning_rate,\n",
    "            global_step = global_step,\n",
    "            decay_steps = decay_steps,\n",
    "            decay_rate = learning_rate_decay_factor,\n",
    "            staircase = True)\n",
    "\n",
    "        #Now we can define the optimizer that takes on the learning rate\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "\n",
    "        #Create the train_op.\n",
    "        train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "        #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
    "        predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "        probabilities = end_points['Predictions']\n",
    "        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "        metrics_op = tf.group(accuracy_update, probabilities)\n",
    "\n",
    "\n",
    "        #Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "        tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        tf.summary.scalar('learning_rate', lr)\n",
    "        my_summary_op = tf.summary.merge_all()\n",
    "        \n",
    "\n",
    "        #Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n",
    "        def train_step(sess, train_op, global_step):\n",
    "            '''\n",
    "            Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n",
    "            '''\n",
    "            #Check the time for each sess run\n",
    "            start_time = time.time()\n",
    "            total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n",
    "            time_elapsed = time.time() - start_time\n",
    "\n",
    "            #Run the logging to print some results\n",
    "            print('global step %s: loss: %.4f (%.2f sec/step)' %( global_step_count, total_loss, time_elapsed))\n",
    "            return total_loss, global_step_count\n",
    "\n",
    "        #Now we create a saver function that actually restores the variables from a checkpoint file in a sess\n",
    "        #write_version=tf.train.SaverDef.V1# this is  for V1\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        \n",
    "        def restore_fn(sess):\n",
    "            return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        #Define your supervisor for running a managed session. Do not run the summary_op automatically or else it will consume too much memory\n",
    "        sv = tf.train.Supervisor(logdir = log_dir, summary_op = None, init_fn = restore_fn)\n",
    "\n",
    "         \n",
    "        #Run the managed session\n",
    "        with sv.managed_session() as sess:\n",
    "\n",
    "\n",
    "            for step in range(num_steps_per_epoch * num_epochs):\n",
    "                print(step)\n",
    "                #At the start of every epoch, show the vital information:\n",
    "                if (step % num_batches_per_epoch) == 0:\n",
    "                    print('Epoch %s/%s' %(step/num_batches_per_epoch + 1, num_epochs))\n",
    "                    learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n",
    "                    print('Current Learning Rate: %s' %(learning_rate_value))\n",
    "                    print('Current Streaming Accuracy: %s' %(accuracy_value))\n",
    "\n",
    "                    # optionally, print your logits and predictions for a sanity check that things are going fine.\n",
    "                    logits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\n",
    "                    print( 'logits: \\n', logits_value)\n",
    "                    print( 'Probabilities: \\n', probabilities_value)\n",
    "                    print( 'predictions: \\n', predictions_value)\n",
    "                    print( 'Labels:\\n:', labels_value)\n",
    "\n",
    "                #Log the summaries every 10 step.\n",
    "                if (step % 10) == 0:\n",
    "                    print('step % 10==0')\n",
    "                    loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "                    summaries = sess.run(my_summary_op)\n",
    "                    sv.summary_computed(sess, summaries)\n",
    "                    \n",
    "                #If not, simply run the training step\n",
    "                else:\n",
    "                    loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "\n",
    "            #We log the final training loss and accuracy\n",
    "            print('Final Loss: %s' %loss)\n",
    "            print('Final Accuracy: %s' %sess.run(accuracy))\n",
    "                          \n",
    "\n",
    "            #Once all the training has been done, save the log files and checkpoint model\n",
    "            print('Finished training! Saving model to disk now.')\n",
    "            \n",
    "            #sv.saver.save(sess, './model.ckpt', global_step = sv.global_step)\n",
    "            \n",
    "            sv.saver.save(sess, sv.save_path, global_step = sv.global_step)# this is  for V2\n",
    "            \n",
    "\n",
    "run()     \n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.python.framework import graph_io\n",
    "#frozen = tf.graph_util.convert_variables_to_const#ants(sess, sess.graph_def, [\"<name_of_the_output_node>\"])\n",
    "#graph_io.write_graph(frozen, './', 'inference_graph.pb', as_text=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print (\"Current date and time : \")\n",
    "print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! example :: freeze_graph.py --input_graph=./code/V12/graph.pbtxt  --input_checkpoint=./code/V12/model.ckpt-0 --input_binary=false   --output_graph=./code/V12/frozen_graph.pb --output_node_names=InceptionResnetV2/Logits/Predictions\n",
    "# EXample \"\" mvNCCompile -s 12  ./frozen_model_inception_resnet_v2.pb -in=Placeholder_only -on=InceptionResnetV2/Logits/Predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "import inception_preprocessing\n",
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "import time\n",
    "import os\n",
    "#from train_flowers import load_batch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "#State your log directory where you can retrieve your model\n",
    "log_dir = './out001/'\n",
    "\n",
    "#Create a new evaluation log directory to visualize the validation process\n",
    "log_eval = './log_eval_test'\n",
    "\n",
    "#State the dataset directory where the validation set is found\n",
    "dataset_dir = datasets_data_dir\n",
    "\n",
    "#State the batch_size to evaluate each time, which can be a lot more than the training batch\n",
    "batch_size = 100\n",
    "\n",
    "#State the number of epochs to evaluate\n",
    "num_epochs = 50\n",
    "\n",
    "#Get the latest checkpoint file\n",
    "checkpoint_file = tf.train.latest_checkpoint(log_dir)\n",
    "file_pattern = 'smotfrecord_validation_%s.tfrecord'\n",
    "\n",
    "def main():\n",
    "    #Create log_dir for evaluation information\n",
    "    if not os.path.exists(log_eval):\n",
    "        os.mkdir(log_eval)\n",
    "\n",
    "    #Just construct the graph from scratch again\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.logging.set_verbosity(tf.logging.INFO)\n",
    "        #Get the dataset first and load one batch of validation images and labels tensors. Set is_training as False so as to use the evaluation preprocessing\n",
    "      \n",
    "        dataset = get_split('validation', datasets_data_dir, file_pattern=file_pattern)\n",
    "        print(dataset.data_sources)\n",
    "        \n",
    "        #image, raw_images, labels = load_batch(dataset, batch_size=batch_size)\n",
    "        image, raw_images, labels = load_batch(dataset, batch_size = batch_size, is_training = False)\n",
    " \n",
    "        \n",
    "        #Create some information about the training steps\n",
    "        num_batches_per_epoch = dataset.num_samples / batch_size\n",
    "        num_steps_per_epoch = num_batches_per_epoch\n",
    "\n",
    "        #Now create the inference model but set is_training=False\n",
    "        with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "            logits, end_points = inception_resnet_v2(image, num_classes = num_classes, is_training = False)\n",
    "\n",
    "        # #get all the variables to restore from the checkpoint file and create the saver function to restore\n",
    "        variables_to_restore = slim.get_variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        def restore_fn(sess):\n",
    "            return saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        #Just define the metrics to track without the loss or whatsoever\n",
    "        predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "        metrics_op = tf.group(accuracy_update)\n",
    "\n",
    "        #Create the global step and an increment op for monitoring\n",
    "        global_step = get_or_create_global_step()\n",
    "        global_step_op = tf.assign(global_step, global_step + 1) #no apply_gradient method so manually increasing the global_step\n",
    "        \n",
    "        #Create a evaluation step function\n",
    "        def eval_step(sess, metrics_op, global_step):\n",
    "           \n",
    "            #Simply takes in a session, runs the metrics op and some logging information.\n",
    "           \n",
    "            start_time = time.time()\n",
    "            _, global_step_count, accuracy_value = sess.run([metrics_op, global_step_op, accuracy])\n",
    "            time_elapsed = time.time() - start_time\n",
    "\n",
    "            #Log some information\n",
    "            print('Global Step %s: Streaming Accuracy: %.4f (%.2f sec/step)' %(global_step_count, accuracy_value, time_elapsed))\n",
    "\n",
    "            return accuracy_value\n",
    "\n",
    "\n",
    "        #Define some scalar quantities to monitor\n",
    "        tf.summary.scalar('Validation_Accuracy', accuracy)\n",
    "        my_summary_op = tf.summary.merge_all()\n",
    "\n",
    "        #Get your supervisor\n",
    "        sv = tf.train.Supervisor(logdir = log_eval, summary_op = None, saver = None, init_fn = restore_fn)\n",
    "\n",
    "        #Now we are ready to run in one session\n",
    "        print(num_steps_per_epoch)\n",
    "        print(num_epochs)\n",
    "                \n",
    "        with sv.managed_session() as sess:\n",
    "            for step in range( num_epochs):\n",
    "                \n",
    "                sess.run(sv.global_step)\n",
    "                #print vital information every start of the epoch as always\n",
    "                if (step % num_batches_per_epoch) == 0:\n",
    "                    print('Epoch: %s/%s' %(step / num_batches_per_epoch + 1, num_epochs))\n",
    "                    \n",
    "                    print('Current Streaming Accuracy: %f' %(sess.run(accuracy)))\n",
    "                    \n",
    "                #Compute summaries every 10 steps and continue evaluating\n",
    "                if (step % 10) == 0:\n",
    "                    eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n",
    "                    summaries = sess.run(my_summary_op)\n",
    "                    sv.summary_computed(sess, summaries)\n",
    "                    \n",
    "\n",
    "                #Otherwise just run as per normal\n",
    "                else:\n",
    "                    eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n",
    "\n",
    "            #At the end of all the evaluation, show the final accuracy\n",
    "            \n",
    "             #Now we want to visualize the last batch's images just to see what our model has predicted\n",
    "            raw_images, labels, predictions = sess.run([raw_images, labels, predictions])\n",
    "            for i in range(99):\n",
    "                \n",
    "                image, label, prediction = raw_images[i], labels[i], predictions[i]\n",
    "                print('index = %d' %(i))\n",
    "               \n",
    "                print('labels= %d predictions= %d ' %(labels[i] ,predictions[i]))\n",
    "                \n",
    "                prediction_name = dataset.labels_to_name[prediction],\n",
    "                label_name = dataset.labels_to_name[label]\n",
    "                \n",
    "          \n",
    "                print(label)\n",
    "                print(prediction)\n",
    "                label_name = dataset.labels_to_name[label]\n",
    "                print('label_name: %s' %(label_name))\n",
    "                prediction_name = dataset.labels_to_name[prediction]\n",
    "                print('Prediction: %s' %(prediction_name))\n",
    "               \n",
    "                #text =('Prediction: %s \\n Ground Truth: %s' %(prediction_name, label_name))\n",
    "                print('Prediction: %s \\n Ground Truth: %s' %(prediction_name, label_name))\n",
    "                \n",
    "                img_plot = plt.imshow(image)\n",
    "\n",
    "                #Set up the plot and hide axes\n",
    "                #plt.title(text)\n",
    "                img_plot.axes.get_yaxis().set_ticks([])\n",
    "                img_plot.axes.get_xaxis().set_ticks([])\n",
    "                plt.show()\n",
    "\n",
    "            \n",
    "            print('Model evaluation has completed! Visit TensorBoard for more information regarding your evaluation.')\n",
    "       \n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "from tensorflow.python.framework import graph_util\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "def CreatePbFile():\n",
    "    image_size = 299\n",
    "    num_classes = 2\n",
    "\n",
    "    \n",
    "    checkpoint_file = tf.train.latest_checkpoint(log_dir)\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        \n",
    "        images = tf.placeholder(shape=[None, image_size, image_size, 3], dtype=tf.float32, name = 'Placeholder_only')\n",
    "\n",
    "        with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "            logits, end_points = inception_resnet_v2(images, num_classes = num_classes, is_training = False)\n",
    "\n",
    "        variables_to_restore = slim.get_variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "        #Setup graph def\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        output_node_names = \"InceptionResnetV2/Logits/Predictions\"\n",
    "        output_graph_name = \"./frozen_model_inception_resnet_v2_1218.pb\"\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, checkpoint_file)\n",
    "\n",
    "            #Exporting the graph\n",
    "            print (\"Exporting graph...\")\n",
    "            output_graph_def = graph_util.convert_variables_to_constants(\n",
    "                sess,\n",
    "                input_graph_def,\n",
    "                output_node_names.split(\",\"))\n",
    "\n",
    "            with tf.gfile.GFile(output_graph_name, \"wb\") as f:\n",
    "                f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "CreatePbFile()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
